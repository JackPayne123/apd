# --- WandB ---
# wandb_project: spd-lm # Project name for Weights & Biases
wandb_project: null # Project name for Weights & Biases
wandb_run_name: null # Set specific run name (optional, otherwise generated)
wandb_run_name_prefix: "" # Prefix for generated run name

# --- General ---
seed: 0
unit_norm_matrices: false # Whether to enforce unit norm on A matrices (not typically used here)
m: 3 # Rank of the decomposition / number of components per layer

# --- Loss Coefficients ---
# Set coeffs to null if the loss shouldn't be computed
param_match_coeff: 1.0
out_recon_coeff: 0.0 # Reconstruction loss based on output logits (MSE)
lp_sparsity_coeff: 0.0 # Coefficient for Lp sparsity loss (applied to component params A & B)
pnorm: 1.0 # p-value for the Lp sparsity norm (1.0 for L1)

# Placeholder losses (set coeffs to null as they require mask calculation implementation)
masked_recon_coeff: null # Reconstruction loss using masks
act_recon_coeff: null # Reconstruction loss on intermediate component activations
random_mask_recon_coeff: null # Reconstruction loss averaged over random masks
layerwise_recon_coeff: null # Layer-wise reconstruction loss
layerwise_random_recon_coeff: 1 # Layer-wise reconstruction loss with random masks

n_random_masks: 1 # Number of random masks if random_mask_recon_coeff is used
n_gate_hidden_neurons: null # Not applicable as there are no gates currently

# --- Training ---
batch_size: 2 # Adjust based on GPU memory
steps: 10_000 # Total training steps
lr: 1e-4 # Learning rate
lr_schedule: cosine # LR schedule type (constant, linear, cosine, exponential)
lr_warmup_pct: 0.01 # Percentage of steps for linear LR warmup
lr_exponential_halflife: null # Required if lr_schedule is exponential
init_from_target_model: false # Not implemented/applicable for this setup

# --- Logging & Saving ---
image_freq: 1000 # Frequency for generating/logging plots
print_freq: 100 # Frequency for printing logs to console
save_freq: 2000 # Frequency for saving checkpoints
image_on_first_step: true # Whether to log plots at step 0

# --- Task Specific ---
task_config:
  task_name: lm # Specifies the LM decomposition task
  model_size: "1.25M" # SimpleStories model size (e.g., "1.25M", "5M", "11M", "30M", "35M")
  max_seq_len: 512 # Maximum sequence length for truncation/padding
  buffer_size: 1000 # Buffer size for streaming dataset shuffling
  dataset_name: "lennart-finke/SimpleStories" # HuggingFace dataset name
  dataset_split: "train" # Dataset split to use
  # List of fnmatch patterns for nn.Linear modules to decompose
  target_module_patterns: ["transformer.h.*.mlp.gate_proj", "transformer.h.*.mlp.up_proj"]
  # Example: Decompose only gate_proj: ["transformer.h.*.mlp.gate_proj"]
  # Example: Decompose gate_proj and up_proj: ["transformer.h.*.mlp.gate_proj", "transformer.h.*.mlp.up_proj"]
  # Example: Decompose all MLP layers: ["transformer.h.*.mlp.*_proj"] 