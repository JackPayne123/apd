# --- WandB ---
wandb_project: spd-lm
# wandb_project: null # Project name for Weights & Biases
wandb_run_name: null # Set specific run name (optional, otherwise generated)
wandb_run_name_prefix: "" # Prefix for generated run name

# --- General ---
seed: 0
unit_norm_matrices: false # Whether to enforce unit norm on A matrices (not typically used here)
m: 20000 # Rank of the decomposition / number of components per layer

# --- Loss Coefficients ---
# Set coeffs to null if the loss shouldn't be computed
param_match_coeff: 1e4
lp_sparsity_coeff: 1e-4 # Coefficient for Lp sparsity loss (applied to component params A & B)
pnorm: 2.0 # p-value for the Lp sparsity norm
# layerwise_random_recon_coeff: null # Layer-wise reconstruction loss with random masks
layerwise_random_recon_coeff: 1.0 # Layer-wise reconstruction loss with random masks
# embedding_recon_coeff: 1  # Custom loss for testing the embedding reconstruction

# Placeholder losses (set coeffs to null as they require mask calculation implementation)
masked_recon_coeff: null # Reconstruction loss using masks
act_recon_coeff: null # Reconstruction loss on intermediate component activations
random_mask_recon_coeff: null # Reconstruction loss averaged over random masks
layerwise_recon_coeff: null # Layer-wise reconstruction loss

n_random_masks: 1 # Number of random masks if random_mask_recon_coeff is used
n_gate_hidden_neurons: 16

# --- Training ---
batch_size: 4 # Adjust based on GPU memory
steps: 50_000 # Total training steps
lr: 1e-3 # Learning rate
lr_schedule: constant # LR schedule type (constant, linear, cosine, exponential)
lr_warmup_pct: 0.01 # Percentage of steps for linear LR warmup
lr_exponential_halflife: null # Required if lr_schedule is exponential
init_from_target_model: false # Not implemented/applicable for this setup

# --- Logging & Saving ---
image_freq: 1000 # Frequency for generating/logging plots
print_freq: 100 # Frequency for printing logs to console
save_freq: 50_000 # Frequency for saving checkpoints
image_on_first_step: true # Whether to log plots at step 0

# --- Task Specific ---
task_config:
  task_name: lm # Specifies the LM decomposition task
  model_size: "1.25M" # SimpleStories model size (e.g., "1.25M", "5M", "11M", "30M", "35M")
  max_seq_len: 512 # Maximum sequence length for truncation/padding
  buffer_size: 1000 # Buffer size for streaming dataset shuffling
  dataset_name: "lennart-finke/SimpleStories" # HuggingFace dataset name
  train_data_split: "train" # Dataset split to use
  eval_data_split: "test" # Dataset split to use
  n_eval_steps: 100 # Number of evaluation steps
  # List of fnmatch patterns for nn.Linear modules to decompose
  # target_module_patterns: ["transformer.h.0.mlp.gate_proj"]
  target_module_patterns: ["transformer.wte"]
  # Example: Decompose only gate_proj: ["transformer.h.*.mlp.gate_proj"]
  # Example: Decompose only the token embedding: ["transformer.wte"]
  # Example: Decompose gate_proj and up_proj: ["transformer.h.*.mlp.gate_proj", "transformer.h.*.mlp.up_proj"]
  # Example: Decompose all MLP layers: ["transformer.h.*.mlp.*_proj"] 

# Config details for the target model taken from https://github.com/danbraunai/simple_stories_train/blob/main/simple_stories_train/models/model_configs.py#L54
  # "1.25M": LlamaConfig(
  #     block_size=512,
  #     vocab_size=4096,
  #     n_layer=4,
  #     n_head=4,
  #     n_embd=128,
  #     n_intermediate=128 * 4 * 2 // 3 = 341,
  #     rotary_dim=128 // 4 = 32,
  #     n_ctx=512,
  #     n_key_value_heads=2,
  #     flash_attention=True,
  # ),